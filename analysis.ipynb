{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# encoding: utf-8\n",
    "%matplotlib inline\n",
    "\n",
    "##processing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "###plotting dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "##text processing dependencies\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import unidecode\n",
    "import re\n",
    "from stopwords import *\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "\n",
    "##similarity metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import MDS\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "###text vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "###clustering algorithms\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import kmeans,vq\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pickle.load( open( \"sample.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = [x.decode('utf-8') for x in set(stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##pre-processing helper functions\n",
    "\n",
    "\n",
    "def cut_text(text) :\n",
    "    return \",\".join(jieba.cut_for_search(text, HMM=True))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \"\".join([x for x in text if x not in stopwords])\n",
    "\n",
    "\n",
    "def remove_ascii(text):\n",
    "    exclude = set(str(string.punctuation + string.digits + string.ascii_uppercase + string.ascii_lowercase))\n",
    "    return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "def remove_unicode(text):\n",
    "    return re.sub(ur\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！\\\n",
    "                    ，。？、~@#￥%……&*（）：；《）《》“”(<>)»〔〕-]+\", \"\", text)\n",
    "\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    stopwords_removed = remove_stopwords(text)\n",
    "    removed_ascii = remove_ascii(stopwords_removed)\n",
    "    tokenized = cut_text(removed_ascii)\n",
    "    return remove_unicode(tokenized)\n",
    "\n",
    "\n",
    "def clean_df(df):\n",
    "    df['text'] = df['text'].map(process_text)\n",
    "    df = df.drop(df[df['text'].map(lambda x : x == '转发微博'.decode('utf-8') or x == '轉發微博'.decode('utf-8'))].index)\n",
    "    df = df.drop(df[df['text'].map(len) < 2].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = clean_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###function pipeline\n",
    "\n",
    "\n",
    "##create tf-idf matrix\n",
    "def tfidf(data):\n",
    "    ##instantiate classifier\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=.01, max_df=.8, ngram_range=(1,2))\n",
    "    #generate matrix\n",
    "    k = tfidf_vectorizer.fit_transform(data)\n",
    "    return tfidf_vectorizer.fit_transform(data) #fit the vectorizer to synopses\n",
    "\n",
    "\n",
    "###run k-means on tf-idf matrix\n",
    "def k_means(data, num_clusters):\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    km.fit(data)\n",
    "    clusters = km.labels_.tolist()\n",
    "    return km, clusters\n",
    "\n",
    "##extract feature words\n",
    "def feature_terms(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=.99, min_df=.0001, ngram_range=(1,3))\n",
    "    #tfidf_vectorizer = TfidfVectorizer(max_df=.8, min_df=.01, ngram_range=(1,2))\n",
    "    vectors = tfidf_vectorizer.fit(data)\n",
    "    return tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "##find the terms with highest tf-idf score\n",
    "def get_top_terms(km, review_terms, num_clusters):\n",
    "    print(\"Top terms per cluster:\")\n",
    "    print()\n",
    "\n",
    "    #sort cluster centers by proximity to centroid\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "    for i in range(1,num_clusters):\n",
    "        print(\"Cluster %d words:\" % i, end='')\n",
    "\n",
    "        for ind in order_centroids[i, :10]: #replace 11 with n words per cluster\n",
    "            print(' %s' % review_terms[ind], end=',')\n",
    "        print() #add whitespace\n",
    "        print() #add whitespace\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "\n",
    "##run functions sequentially  \n",
    "def pipeline(text, n_clusters):\n",
    "    matrix = tfidf(text)\n",
    "    km, clusters = k_means(matrix, n_clusters)\n",
    "    features = feature_terms(text)\n",
    "    terms = get_top_terms(km, features, n_clusters)\n",
    "    return terms\n",
    "\n",
    "\n",
    "pipeline(df.text, n_clusters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pca(text):\n",
    "    X = tfidf(text).todense()\n",
    "    \n",
    "    color = ['r','b','g']\n",
    "\n",
    "    pca = PCA(n_components=3).fit(X)\n",
    "    data2D = pca.transform(X)\n",
    "    plt.scatter(data2D[:,0], data2D[:,1], c=color)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title(\"K-means Principal Components\")\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca(df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lda_pipe(data, n_topics, n_top_words):\n",
    "    \n",
    "    n_features = 100\n",
    "    data_samples = data\n",
    "    n_topics = n_topics\n",
    "    n_top_words = n_top_words\n",
    "    \n",
    "    \n",
    "\n",
    "    def print_top_words(model, feature_names, n_top_words):\n",
    "        terms = []\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            print(\"Topic #%d:\" % topic_idx)\n",
    "            print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words:-1]]))\n",
    "            terms.append(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words:-1]]))\n",
    "        return terms\n",
    "        \n",
    "    def lda_dict(model, feature_names, n_top_words):\n",
    "        lda_dict = dict()\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            print(\"Topic #%d:\" % topic_idx)\n",
    "            features, rank = [],[]\n",
    "            features.append(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words:-1]]))\n",
    "            rank.append(topic[:-n_top_words-1:-1])\n",
    "            d = dict(zip(features,rank))\n",
    "            lda_dict[topic_idx] = {k:v for k,v in d.items()}\n",
    "        return lda_dict\n",
    "\n",
    "        \n",
    "\n",
    "    print(\"Extracting tf features for LDA...\")\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=.01, max_features=n_features)\n",
    "\n",
    "    tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=10,\n",
    "                                    learning_method='online', learning_offset=50.,\n",
    "                                    random_state=1)\n",
    "\n",
    "    lda.fit(tf)\n",
    "\n",
    "\n",
    "    print(\"\\nTopics in LDA model:\")\n",
    "    print()\n",
    "    print()\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    print_top_words(lda, tf_feature_names, n_top_words)\n",
    "    \n",
    "    \n",
    "lda_pipe(weibo.text, 6, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##modify function for application\n",
    "\n",
    "def lda_generator(data_samples, n_top_words, n_topics):\n",
    "    n_features = 100\n",
    "    data_samples = data_samples\n",
    "    n_topics = n_topics\n",
    "    n_top_words = n_top_words\n",
    "    \n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=.01,ngram_range=(1,2), max_features=n_features)\n",
    "    \n",
    "    tf_matrix = tf_vectorizer.fit_transform(data_samples)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=10,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=1)\n",
    "    lda.fit(tf_matrix)\n",
    "    \n",
    "    feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    terms_zh = []\n",
    "\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        lda_topics = (\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words:-1]]))\n",
    "        print(lda_topics)\n",
    "        terms_zh.append(lda_topics)\n",
    "        \n",
    "    return terms_zh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = lda_generator(weibo.text, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot elbows and K-means clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_kmeans(data):\n",
    "    tfidf = TfidfVectorizer(min_df=.01, ngram_range=(1,2))\n",
    "    X = tfidf.fit_transform(data).todense()\n",
    "\n",
    "    ##### cluster data into K=1..10 clusters #####\n",
    "    K = range(1,10)\n",
    "\n",
    "    # scipy kmeans module for each value of k:\n",
    "    KM = [kmeans(X,k) for k in K]\n",
    "    ##list comprehension to cluster centroids\n",
    "    centroids = [cent for (cent,var) in KM]\n",
    "\n",
    "\n",
    "    # alternative: scipy.spatial.distance.cdist\n",
    "    D_k = [cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "    cIdx = [np.argmin(D,axis=1) for D in D_k]\n",
    "    dist = [np.min(D,axis=1) for D in D_k]\n",
    "    avgWithinSS = [sum(d)/X.shape[0] for d in dist]\n",
    "\n",
    "    ##### plot ###\n",
    "    kIdx = 4\n",
    "\n",
    "    # elbow curve\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(K, avgWithinSS, 'b*-')\n",
    "    ax.plot(K[kIdx], avgWithinSS[kIdx], marker='x', markersize=12, \n",
    "        markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Average within-cluster sum of squares')\n",
    "    plt.title('Elbow for KMeans clustering')\n",
    "\n",
    "    # scatter plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    #ax.scatter(X[:,2],X[:,1], s=30, c=cIdx[k])\n",
    "    clr = ['b','g','r','c','m','y','k']\n",
    "    for i in range(K[kIdx]):\n",
    "        ind = (cIdx[kIdx]==i)\n",
    "        ax.scatter(X[ind,1],X[ind,2], s=30, c=clr[i], label='Cluster %d'%i)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Censored Tweet Clusters with K=%d' % K[kIdx])\n",
    "    plt.legend()\n",
    "\n",
    "plot_kmeans(df.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [weibos]",
   "language": "python",
   "name": "Python [weibos]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
